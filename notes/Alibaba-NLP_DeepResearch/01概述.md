
Tongyi DeepResearch 是一个大型语言模型，具有 300 亿个总参数，每个令牌只有 30 亿个参数的稀疏激活。该系统专为长期、深度信息搜索任务而设计，并在多个代理搜索基准测试中展示了最先进的性能，包括 Humanity's Last Exam、BrowserComp、WebWalkerQA、xbench-DeepSearch、FRAMES 和 SimpleQA。
该系统建立在 WebAgent 项目的基础上，并结合了一个完全自动化的合成数据生成管道，大规模的持续预训练，以及使用组相对策略优化的端到端强化学习。

## 核心系统架构
### 1. 高级组件结构
```mermaid
graph TB
    Model["Tongyi-DeepResearch-30B-A3B<br/>30B params, 3B activated"]

    InferenceScript["run_react_infer.sh<br/>Main Execution Script"]
    InferenceDir["inference/<br/>Inference Pipeline"]
    EvalData["eval_data/<br/>Evaluation Datasets"]

    ReactMode["ReAct Mode<br/>Intrinsic Ability Evaluation"]
    HeavyMode["IterResearch Heavy Mode<br/>Maximum Performance"]

    ExternalTools["External Tool Integration"]
    RetrievalAPI["Retrieval APIs"]
    Calculator["Calculator"]
    WebSearch["Web Search"]
    FileParser["File Parser"]

    EvalScripts["evaluation/<br/>Benchmark Scripts"]

    Model --> InferenceScript
    InferenceScript --> InferenceDir
    InferenceScript --> EvalData

    InferenceDir --> ReactMode
    InferenceDir --> HeavyMode

    ReactMode --> ExternalTools
    HeavyMode --> ExternalTools

    ExternalTools --> RetrievalAPI
    ExternalTools --> Calculator
    ExternalTools --> WebSearch
    ExternalTools --> FileParser

    InferenceScript --> EvalScripts
```


### 2. 训练和数据管道
```mermaid
graph LR
    SyntheticData["Fully Automated<br/>Synthetic Data Generation<br/>全自动合成数据生成"]
    ContinualPT["Large-scale<br/>Continual Pre-training<br/>大规模持续预训练"]
    RL["Group Relative Policy Optimization RL<br/>群组相对策略优化强化学习"]

    AgenticData["Agentic Interaction Data<br/>智能体交互数据</b>"]
    SFT["Supervised Fine-tuning<br/>监督微调"]
    TokenLevel["Token-level<br/>Policy Gradients<br/>Token级策略梯度"]

    SyntheticData --> AgenticData
    AgenticData --> ContinualPT
    AgenticData --> SFT
    SFT --> RL
    RL --> TokenLevel

    ContinualPT --> Model["Tongyi-DeepResearch-30B-A3B"]
    RL --> Model
```
#### 三阶段训练架构
```mermaid
graph TD
    %% 节点：英文原名 + 中文释义
    A["Raw Data Sources<br/><b>原始数据源</b>"]
    --> B["Stage 1: Synthetic Data Generation<br/><b>阶段1：合成数据生成</b>"]

    B --> C["Agentic Interaction Data<br/><b>智能体交互数据</b>"]
    B --> D["Supervised Fine-tuning Data<br/><b>监督微调数据</b>"]
    B --> E["Reinforcement Learning Data<br/><b>强化学习数据</b>"]

    C --> F["Stage 2: Continual Pre-training<br/><b>阶段2：持续预训练</b>"]
    F --> G["Base Model with Agentic Capabilities<br/><b>具备智能体能力的基础模型</b>"]

    D --> H["Stage 3: Supervised Fine-tuning<br/><b>阶段3：监督微调</b>"]
    G --> H
    H --> I["Supervised Fine-tuned Model<br/><b>监督微调后模型</b>"]

    E --> J["Stage 3: Reinforcement Learning<br/><b>阶段3：强化学习</b>"]
    I --> J
    J --> K["Tongyi-DeepResearch-30B-A3B<br/><b>通义深度研究30B-A3B</b>"]

    %% 强化学习所用技术
    L["Group Relative Policy Optimization<br/><b>群组相对策略优化</b>"] --> J
    M["Token-level Policy Gradients<br/><b>Token级策略梯度</b>"] --> J
    N["Leave-one-out Advantage Estimation<br/><b>留一法优势估计</b>"] --> J
    O["Selective Negative Sample Filtering<br/><b>选择性负样本过滤</b>"] --> J
```
**阶段1：自动合成数据生成**
该管道从一个完全自动化的合成数据生成系统开始，该系统为所有后续训练阶段创建不同的训练数据集。这一阶段产生：
- 预训练数据 ：用于持续预训练的大规模交互数据
- 有监督的微调数据 ：高质量的问答对和推理痕迹
- 强化学习数据 ：用于策略优化的事件数据
**第二阶段：持续的pre-training**
持续的预训练阶段使用多样化的高质量代理交互数据扩展基础模型的功能。这一过程：
- 保持数据的新鲜度和相关性
- 增强推理能力
- 扩展模型功能，用于长期任务(long-horizon tasks)
- 保留现有知识，同时增加代理能力
**第三阶段：微调和强化学习**
最后一个阶段将监督微调与端到端强化学习相结合：
- 监督微调 ：使模型与所需的响应模式和推理结构保持一致。
- 强化学习 ：采用具有专用组件的组相对策略优化框架，用于在非静态环境中进行稳定训练。
**强化学习框架详细信息**

| 组件 | 目的 | 执行 |
|---|---|---|
| 组相关策略优化 | 核心 RL 算法 | 定制的代理培训框架 |
| 令牌级策略约束 | 细粒度优化 | 每令牌梯度计算 |
| 留一法优势估计 | 方差减少 | 改进的优势估计 |
| 选择性阴性样本过滤 | 训练稳定性 | 在非平稳环境中过滤有问题的样本 |

Group Relative Policy Optimization Framework
```mermaid
graph TB
    A["策略模型"] --> B["Token级策略梯度"]
    C["参考模型"] --> D["留一法优势估计"]
    E["经验缓冲池"] --> F["群组相对打分"]

    B --> G["策略更新"]
    D --> G
    F --> H["选择性过滤"]

    H --> I["过滤后训练批次"]
    I --> G

    J["非平稳环境处理器"] --> G

    K["合成数据管线"] --> E
    L["智能体交互"] --> E

    G --> M["更新后的策略模型"]
    M --> N["下一轮迭代"]

    subgraph "GRPO 核心组件"
        B
        D
        F
        H
    end

    subgraph "训练稳定性"
        J
        I
    end
```

1) 令牌级策略约束
```mermaid
graph LR
    A["输入序列"] --> B["Token嵌入"]
    B --> C["策略网络前向传播"]
    C --> D["动作概率 π(a|s)"]

    E["奖励信号 R"] --> F["优势计算 A(s,a)"]
    D --> F

    F --> G["Token级梯度 ∇log π(a|s) * A(s,a)"]
    G --> H["策略参数更新"]

    I["参考策略 π_ref"] --> J["KL散度约束"]
    D --> J
    J --> H

    subgraph "逐Token处理"
        D
        F
        G
    end
```
令牌级方法确保以适当的粒度应用策略更新，使模型能够学习细微的代理行为，同时保持连贯的文本生成功能。

2) 留一法优势估计
> 留一法是一种交叉验证思想，每次只把一个样本“留”出来做验证或计算，其余全部样本用来训练或估计基线。

该框架采用了留一法的优势估计策略，为策略优化提供稳定和无偏的优势估计。这种技术有助于缓解通常与策略梯度方法相关的高方差。
优势评估流程:
```mermaid
graph TB
    A["经验批次 {x1, x2, ..., xN}"] --> B["构建群组"]
    
    B --> C["对每个样本 xi："]
    C --> D["生成留一群组 G-i = {x1,...,xi-1,xi+1,...,xN}"]
    
    D --> E["计算基线 b(G-i)"]
    E --> F["求优势 A(xi) = R(xi) - b(G-i)"]
    
    F --> G["优势估计集合 {A(x1), A(x2), ..., A(xN)}"]
    
    H["奖励函数 R"] --> F
    I["价值函数 V"] --> E
    
    subgraph "留一法逻辑"
        D
        E
        F
    end
    
    G --> J["策略梯度计算"]
```
这种估计方法通过使用每个批次中的剩余样本来建立动态基线来减少方差，从而导致更稳定的训练动态。

3) 选择性过滤机制
该框架采用了选择性过滤机制来处理负样本，并在非平稳环境中保持训练稳定性。此组件过滤掉可能会降低策略性能的潜在有害或低质量体验。
过滤管道
```mermaid
graph TD
    A["原始经验样本"] --> B["质量评估"]

    B --> C["奖励阈值检查"]
    B --> D["一致性校验"]
    B --> E["多样性分析"]

    C --> F["样本分类"]
    D --> F
    E --> F

    F --> G["高质量样本"]
    F --> H["低质量样本"]
    F --> I["负样本"]

    G --> J["加入训练"]
    H --> K["条件加入"]
    I --> L["过滤掉"]

    K --> M["额外校验"]
    M --> J
    M --> L

    subgraph "过滤标准"
        C
        D
        E
    end

    subgraph "训练批次构成"
        J
        L
    end
```
选择性过滤确保只有高质量的经验有助于政策更新，防止在复杂的代理环境中可能发生的负面样本的退化。

4) 非平稳环境镇定
GRPO 框架解决了在非平稳环境中训练的挑战，在非平稳环境中，最优策略可能会随着模型学习和环境动态的演变而发生变化。
稳定策略
```mermaid
graph LR
    A["环境状态跟踪"] --> B["分布偏移检测"]
    B --> C["自适应学习率"]

    D["策略散度监控"] --> E["KL约束调整"]
    E --> F["训练稳定性"]

    G["经验回放池"] --> H["时序一致性检查"]
    H --> I["样本重加权"]

    C --> J["稳定策略更新"]
    F --> J
    I --> J

    K["参考模型更新"] --> L["周期同步"]
    L --> J

    subgraph "稳定性机制"
        C
        F
        I
        L
    end
```

GRPO 框架被实现为一个端到端系统，可以处理代理强化学习的复杂性，同时保持计算效率和训练稳定性。

| 组件 | 功能 | 关键特征 |
|---|---|---|
| 令牌级策略约束 | 细粒度优化 | 每代币奖励属性，梯度精度 |
| 留一法优势 | 方差减少 | 动态基线，无偏估计 |
| 选择性滤波 | 样品质量控制 | 阴性样品处理，一致性验证 |
| 环境稳定 | 非平稳性处理 | 自适应参数、分布监测 |

该框架的 on-policy 方法确保策略更新基于当前策略行为，避免了语言模型训练的 off-policy 方法中常见的分布不匹配问题。

---
## Agent Family 集成体系结构
```mermaid
graph TB
    %% 节点定义：英文标题 + 中文解释
    WebAgent["WebAgent<br/><b>网页智能体·基础项目</b>"]
    TongyiDR["Tongyi DeepResearch<br/><b>通义深度研究·中央系统</b>"]

    NavAgents["Navigation Agents<br/><b>导航型智能体</b>"]
    InfoAgents["Information Seeking Agents<br/><b>信息检索智能体</b>"]
    VisionAgents["Vision-Language Agents<br/><b>视觉-语言智能体</b>"]
    DataAgents["Data Synthesis Agents<br/><b>数据合成智能体</b>"]
    FoundationAgents["Foundation Agents<br/><b>基础能力智能体</b>"]

    WebWalker["WebWalker<br/><b>网页漫步者</b>"]
    WebSailor["WebSailor<br/><b>网页水手</b>"]
    WebSailorV2["WebSailor-V2<br/><b>网页水手 V2</b>"]

    WebDancer["WebDancer<br/><b>网页舞者</b>"]
    WebResearcher["WebResearcher<br/><b>网页研究员</b>"]
    WebWeaver["WebWeaver<br/><b>网页织工</b>"]

    WebWatcher["WebWatcher<br/><b>网页观察者</b>"]

    WebShaper["WebShaper<br/><b>网页塑造者</b>"]
    ReSum["ReSum / WebResummer<br/><b>网页摘要器</b>"]

    AgentFounder["AgentFounder<br/><b>智能体奠基者·持续预训练</b>"]
    AgentScaler["AgentScaler<br/><b>智能体扩展·环境规模化</b>"]

    %% 连线关系
    WebAgent --> TongyiDR
    TongyiDR --> NavAgents
    TongyiDR --> InfoAgents
    TongyiDR --> VisionAgents
    TongyiDR --> DataAgents
    TongyiDR --> FoundationAgents

    NavAgents --> WebWalker
    NavAgents --> WebSailor
    NavAgents --> WebSailorV2

    InfoAgents --> WebDancer
    InfoAgents --> WebResearcher
    InfoAgents --> WebWeaver

    VisionAgents --> WebWatcher

    DataAgents --> WebShaper
    DataAgents --> ReSum

    FoundationAgents --> AgentFounder
    FoundationAgents --> AgentScaler
```
















